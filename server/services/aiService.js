const Groq = require("groq-sdk");

const client = new Groq({
  apiKey: process.env.GROQ_API_KEY,
});

console.log("ğŸ”‘ Groq API Key exists:", !!process.env.GROQ_API_KEY);

exports.generateFileIntelligence = async (code) => {
  console.log("ğŸ¤– generateFileIntelligence: Starting...");
  console.log("ğŸ“ Code length:", code.length);
  
  const response = await client.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [
      {
        role: "system",
        content:
          "Analyze the following code. Return ONLY a valid JSON object (no markdown, no code blocks) with these fields: purpose, responsibilities, keyFunctions, suggestedQuestions.",
      },
      {
        role: "user",
        content: code,
      },
    ],
  });

  const rawContent = response.choices[0].message.content;
  console.log("âœ… Groq raw response:", rawContent);
  
  // Clean up response - remove markdown code blocks if present
  let cleanedContent = rawContent.trim();
  if (cleanedContent.startsWith("```json")) {
    cleanedContent = cleanedContent.replace(/^```json\n?/, "").replace(/\n?```$/, "");
  } else if (cleanedContent.startsWith("```")) {
    cleanedContent = cleanedContent.replace(/^```\n?/, "").replace(/\n?```$/, "");
  }
  
  console.log("âœ… Cleaned response:", cleanedContent);
  return cleanedContent;
};

exports.answerQuestion = async (intelligence, question) => {
  console.log("ğŸ¤– answerQuestion: Starting...");
  console.log("ğŸ“ Question:", question);
  
  const response = await client.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [
      {
        role: "system",
        content: `You are answering based on this analysis:\n${intelligence}`,
      },
      {
        role: "user",
        content: question,
      },
    ],
  });

  const answer = response.choices[0].message.content;
  console.log("âœ… Groq answer received:", answer);
  return answer;
};

// âœ… FIX: Use the same model as other functions
exports.answerQuestionStream = async function* (path, question) {
  console.log("ğŸ¤– answerQuestionStream: Starting...");
  console.log("ğŸ“ Path:", path, "Question:", question);
  
  const completion = await client.chat.completions.create({
    model: "llama-3.1-8b-instant", // âœ… FIXED: Changed from llama3-70b-8192
    messages: [
      { role: "system", content: `You are analyzing ${path}` },
      { role: "user", content: question }
    ],
    stream: true,
  });

  console.log("âœ… Stream started");
  
  for await (const chunk of completion) {
    const token = chunk.choices[0]?.delta?.content;
    if (token) {
      console.log("ğŸ“¤ Token:", token);
      yield token;
    }
  }
  
  console.log("âœ… Stream complete");
};